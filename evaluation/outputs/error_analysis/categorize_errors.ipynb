{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f543a7d5",
   "metadata": {},
   "source": [
    "Code for sampling errors from GPT-4 and Mistral outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67c0d4081e357358",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T02:16:19.094729Z",
     "start_time": "2024-04-11T02:16:19.091174Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'yaml'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m      2\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../../\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m defaultdict\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dict\n",
      "File \u001b[0;32m~/Documents/nlp/automating_meta_analysis_with_LLMs/llm-meta-analysis/evaluation/outputs/error_analysis/../../utils.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtemplates\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Template\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dict, List, Optional, Any, Tuple\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/nlp/automating_meta_analysis_with_LLMs/llm-meta-analysis/evaluation/outputs/error_analysis/../../templates.py:7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01muuid\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpkg_resources\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01myaml\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjinja2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseLoader, Environment\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'yaml'"
     ]
    }
   ],
   "source": [
    "from evaluate_output import MetaAnalysisTaskEvaluator\n",
    "import utils\n",
    "from collections import defaultdict\n",
    "from typing import Dict\n",
    "import yaml\n",
    "import json\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "423a6f3a134ac5cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T02:16:19.111196Z",
     "start_time": "2024-04-11T02:16:19.097074Z"
    }
   },
   "outputs": [],
   "source": [
    "def categorize_outcome_type(output_file_path: str) -> (Dict, Dict):\n",
    "    \"\"\"\n",
    "    Categorizes the errors in the outcome type task\n",
    "    Categories:\n",
    "        1. Model outputs in an undesirable format\n",
    "        2. Model outputs binary when its continuous\n",
    "        3. Model outputs continuous when its binary\n",
    "        4. Model outputs unknown when the reference is known\n",
    "    \n",
    "    Args:\n",
    "        output_file_path: Output file path of the outcome type task\n",
    "    \n",
    "    Returns:\n",
    "        Dict: A dictionary containing the error categories and the record ids that fall into each category\n",
    "    \"\"\"\n",
    "    \n",
    "    def is_badly_formatted_output(output: str) -> bool:\n",
    "        return output not in [\"A\", \"B\", \"C\"]\n",
    "    \n",
    "    evaluator = MetaAnalysisTaskEvaluator('outcome_type', output_file_path, 'metrics/outcome_type/', None)\n",
    "    evaluator.run_evaluation()\n",
    "    \n",
    "    errors = defaultdict(list)\n",
    "    # character_to_string_mapping = {\"A\": \"binary\", \"B\": \"continuous\", \"C\": \"x\"}\n",
    "    for record in evaluator.data:\n",
    "        \n",
    "        # Clean the output in the same way from `evaluate_output.py`\n",
    "        output = record['output'].replace(\"The answer is \", \"\").replace(\".\", \"\").replace(\"(\", \"\").replace(\")\",\"\")\n",
    "        for char in output:\n",
    "            if not char.isspace():\n",
    "                output = char\n",
    "                break\n",
    "               \n",
    "        # Check for badly formatted outputs\n",
    "        if is_badly_formatted_output(output):\n",
    "            errors[\"badly_formatted_output\"].append(record['id'])\n",
    "            \n",
    "        # Check for unknown when reference is known\n",
    "        elif output == \"C\" and record['outcome_type'] != \"\":\n",
    "            errors[\"unknown_when_reference_known\"].append(record['id'])\n",
    "        \n",
    "        # Check for binary when continuous\n",
    "        elif output == \"B\" and record['outcome_type'] == \"binary\":\n",
    "            errors[\"binary_when_continuous\"].append(record['id'])\n",
    "            \n",
    "        # Check for continuous when binary\n",
    "        elif output == \"A\" and record['outcome_type'] == \"continuous\":\n",
    "            errors[\"continuous_when_binary\"].append(record['id'])\n",
    "            \n",
    "    # Get the records that fall into each category\n",
    "    error_records = {}\n",
    "    for category, ids in errors.items():\n",
    "        error_records[category] = []\n",
    "        for record in evaluator.data:\n",
    "            if record['id'] in ids:\n",
    "                error_records[category].append(record)\n",
    "            \n",
    "    return errors, error_records\n",
    "\n",
    "def categorize_outcomes(output_file_path: str, outcome_type: str) -> (Dict, Dict):\n",
    "    \"\"\"\n",
    "    Categorizes the errors in the binary outcomes task\n",
    "    Categories:\n",
    "        1. Model outputs in an undesirable format\n",
    "        2. Model has output but reference is unknown\n",
    "        3. Model outputs unknown but reference is known\n",
    "        4. Reference is and output is known but model outputs incorrect value\n",
    "        \n",
    "    Args:\n",
    "        output_file_path: Output file path of the given task\n",
    "        outcome_type: The outcome type of the task\n",
    "        \n",
    "    Returns:\n",
    "        Dict: A dictionary containing the error categories and the record ids that fall into each category\n",
    "    \"\"\"\n",
    "    evaluator = MetaAnalysisTaskEvaluator(outcome_type, output_file_path, 'metrics/' + outcome_type + '/', None)\n",
    "    evaluator.run_evaluation()\n",
    "\n",
    "    errors = defaultdict(list)\n",
    "    \n",
    "    # Define the reference keys and output keys based on the outcome type\n",
    "    if outcome_type == 'binary_outcomes':\n",
    "        reference_keys = [\"intervention_events\", \"intervention_group_size\", \"comparator_events\", \"comparator_group_size\"]\n",
    "        output_keys = [f'{category}_output' for category in reference_keys]\n",
    "    elif outcome_type == 'continuous_outcomes':\n",
    "        reference_keys = ['intervention_mean', 'intervention_standard_deviation', 'intervention_group_size', 'comparator_mean', 'comparator_standard_deviation', 'comparator_group_size']\n",
    "        output_keys = [f'{category}_output' for category in reference_keys]\n",
    "    else:\n",
    "        raise ValueError(\"Invalid outcome type\")\n",
    "    \n",
    "    for record in evaluator.data:\n",
    "        # Check for badly formatted outputs\n",
    "        try:\n",
    "            _ = yaml.safe_load(utils.clean_yaml_output(record['output']))\n",
    "        except:\n",
    "            errors[\"badly_formatted_output\"].append(record['id'])\n",
    "            continue\n",
    "            \n",
    "        # Check for output but reference is unknown\n",
    "        for reference_key, output_key in zip(reference_keys, output_keys):\n",
    "            if record[reference_key] == \"x\" and record[output_key] != \"x\":\n",
    "                # Check if the record is already in the list\n",
    "                if record['id'] not in errors[\"known_output_but_reference_unknown\"]:\n",
    "                    errors[\"known_output_but_reference_unknown\"].append(record['id'])\n",
    "                \n",
    "            \n",
    "            # Check for unknown output but reference is known\n",
    "            if record[reference_key] != \"x\" and record[output_key] == \"x\":\n",
    "                # Check if the record is already in the list\n",
    "                if record['id'] not in errors[\"unknown_output_but_reference_known\"]:\n",
    "                    errors[\"unknown_output_but_reference_known\"].append(record['id'])\n",
    "                \n",
    "                \n",
    "            # Check for incorrect answer\n",
    "            if record[reference_key] != \"x\" and record[output_key] != \"x\":\n",
    "                if record[reference_key] != record[output_key]:\n",
    "                    # Check if the record is already in the list\n",
    "                    if record['id'] not in errors[\"incorrect_output\"]:\n",
    "                        errors[\"incorrect_output\"].append(record['id'])\n",
    "                        \n",
    "    # Get the records that fall into each category\n",
    "    error_records = {}\n",
    "    for category, ids in errors.items():\n",
    "        error_records[category] = []\n",
    "        for record in evaluator.data:\n",
    "            if record['id'] in ids:\n",
    "                error_records[category].append(record)\n",
    "                        \n",
    "    return errors, error_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6bbb9fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T02:16:19.118807Z",
     "start_time": "2024-04-11T02:16:19.113591Z"
    }
   },
   "outputs": [],
   "source": [
    "output_files_by_model = {\n",
    "    \"biomistral\": {\n",
    "        \"outcome_type\": \"biomistral_outcome_type_test_output.json\",\n",
    "        \"binary_outcomes\": \"biomistral_binary_outcomes_test_output.json\",\n",
    "        \"continuous_outcomes\": \"biomistral_continuous_outcomes_test_output.json\"\n",
    "    },\n",
    "    \"gemma\": {\n",
    "        \"outcome_type\": \"gemma7B_outcome_type_test_output.json\",\n",
    "        \"binary_outcomes\": \"gemma7B_binary_outcomes_test_output.json\",\n",
    "        \"continuous_outcomes\": \"gemma7B_continuous_outcomes_test_output.json\"\n",
    "    },\n",
    "    \"gpt4\": {\n",
    "        \"outcome_type\": \"gpt4_outcome_type_test_output.json\",\n",
    "        \"binary_outcomes\": \"gpt4_binary_outcomes_test_output.json\",\n",
    "        \"continuous_outcomes\": \"gpt4_continuous_outcomes_test_output.json\"\n",
    "    },\n",
    "    \"gpt35\": {\n",
    "        \"outcome_type\": \"gpt35_outcome_type_test_output.json\",\n",
    "        \"binary_outcomes\": \"gpt35_binary_outcomes_test_output.json\",\n",
    "        \"continuous_outcomes\": \"gpt35_continuous_outcomes_test_output.json\"\n",
    "    },\n",
    "    \"mistral\": {\n",
    "        \"outcome_type\": \"mistral7B_outcome_type_test_output.json\",\n",
    "        \"binary_outcomes\": \"mistral7B_binary_outcomes_test_output.json\",\n",
    "        \"continuous_outcomes\": \"mistral7B_continuous_outcomes_test_output.json\"\n",
    "    },\n",
    "    \"olmo\": {\n",
    "        \"outcome_type\": \"olmo7B_outcome_type_test_output.json\",\n",
    "        \"binary_outcomes\": \"olmo7B_binary_outcomes_test_output.json\",\n",
    "        \"continuous_outcomes\": \"olmo7B_continuous_outcomes_test_output.json\"\n",
    "    },\n",
    "    \"pmc_llama\": {\n",
    "        \"outcome_type\": \"pmc-llama_outcome_type_test_output.json\",\n",
    "        \"binary_outcomes\": \"pmc-llama_binary_outcomes_test_output.json\",\n",
    "        \"continuous_outcomes\": \"pmc-llama_continuous_outcomes_test_output.json\"\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce5c872",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T02:16:23.249875Z",
     "start_time": "2024-04-11T02:16:19.120944Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save the original standard output\n",
    "original_stdout = sys.stdout \n",
    "\n",
    "# # Redirect standard output to a null device\n",
    "sys.stdout = open(os.devnull, 'w')\n",
    "\n",
    "outcome_type_errors_by_model = {}\n",
    "\n",
    "for model, output_files in output_files_by_model.items():\n",
    "    outcome_type_errors = {}\n",
    "    _, outcome_type_errors[\"outcome_type\"] = categorize_outcome_type(f\"outputs/outcome_type/{output_files['outcome_type']}\")\n",
    "    _, outcome_type_errors[\"binary_outcomes\"] = categorize_outcomes(f\"outputs/binary_outcomes/{output_files['binary_outcomes']}\", 'binary_outcomes')\n",
    "    _, outcome_type_errors[\"continuous_outcomes\"] = categorize_outcomes(f\"outputs/continuous_outcomes/{output_files['continuous_outcomes']}\", 'continuous_outcomes')\n",
    "    outcome_type_errors_by_model[model] = outcome_type_errors\n",
    "    \n",
    "# Restore the original standard output\n",
    "sys.stdout = original_stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12b074e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T02:16:23.256959Z",
     "start_time": "2024-04-11T02:16:23.251406Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: biomistral\n",
      "--------------------------------------------------\n",
      "Outcome Type Errors:\n",
      "unknown_when_reference_known: 32\n",
      "badly_formatted_output: 377\n",
      "continuous_when_binary: 149\n",
      "binary_when_continuous: 11\n",
      "\n",
      "Binary Outcomes Errors:\n",
      "unknown_output_but_reference_known: 165\n",
      "\n",
      "Continuous Outcomes Errors:\n",
      "unknown_output_but_reference_known: 465\n",
      "\n",
      "--------------------------------------------------\n",
      "Model: gemma\n",
      "--------------------------------------------------\n",
      "Outcome Type Errors:\n",
      "binary_when_continuous: 13\n",
      "continuous_when_binary: 207\n",
      "\n",
      "Binary Outcomes Errors:\n",
      "incorrect_output: 113\n",
      "unknown_output_but_reference_known: 31\n",
      "known_output_but_reference_unknown: 23\n",
      "badly_formatted_output: 11\n",
      "\n",
      "Continuous Outcomes Errors:\n",
      "incorrect_output: 212\n",
      "unknown_output_but_reference_known: 216\n",
      "badly_formatted_output: 18\n",
      "known_output_but_reference_unknown: 125\n",
      "\n",
      "--------------------------------------------------\n",
      "Model: gpt4\n",
      "--------------------------------------------------\n",
      "Outcome Type Errors:\n",
      "unknown_when_reference_known: 155\n",
      "continuous_when_binary: 25\n",
      "binary_when_continuous: 8\n",
      "\n",
      "Binary Outcomes Errors:\n",
      "incorrect_output: 34\n",
      "known_output_but_reference_unknown: 12\n",
      "unknown_output_but_reference_known: 20\n",
      "\n",
      "Continuous Outcomes Errors:\n",
      "incorrect_output: 113\n",
      "unknown_output_but_reference_known: 142\n",
      "known_output_but_reference_unknown: 38\n",
      "\n",
      "--------------------------------------------------\n",
      "Model: gpt35\n",
      "--------------------------------------------------\n",
      "Outcome Type Errors:\n",
      "continuous_when_binary: 99\n",
      "badly_formatted_output: 11\n",
      "binary_when_continuous: 7\n",
      "unknown_when_reference_known: 141\n",
      "\n",
      "Binary Outcomes Errors:\n",
      "incorrect_output: 69\n",
      "unknown_output_but_reference_known: 58\n",
      "known_output_but_reference_unknown: 18\n",
      "\n",
      "Continuous Outcomes Errors:\n",
      "unknown_output_but_reference_known: 147\n",
      "incorrect_output: 174\n",
      "known_output_but_reference_unknown: 108\n",
      "\n",
      "--------------------------------------------------\n",
      "Model: mistral\n",
      "--------------------------------------------------\n",
      "Outcome Type Errors:\n",
      "unknown_when_reference_known: 480\n",
      "continuous_when_binary: 34\n",
      "badly_formatted_output: 9\n",
      "binary_when_continuous: 1\n",
      "\n",
      "Binary Outcomes Errors:\n",
      "incorrect_output: 123\n",
      "unknown_output_but_reference_known: 7\n",
      "known_output_but_reference_unknown: 25\n",
      "badly_formatted_output: 3\n",
      "\n",
      "Continuous Outcomes Errors:\n",
      "incorrect_output: 226\n",
      "known_output_but_reference_unknown: 171\n",
      "unknown_output_but_reference_known: 171\n",
      "badly_formatted_output: 5\n",
      "\n",
      "--------------------------------------------------\n",
      "Model: olmo\n",
      "--------------------------------------------------\n",
      "Outcome Type Errors:\n",
      "continuous_when_binary: 461\n",
      "unknown_when_reference_known: 5\n",
      "\n",
      "Binary Outcomes Errors:\n",
      "unknown_output_but_reference_known: 12\n",
      "badly_formatted_output: 115\n",
      "incorrect_output: 42\n",
      "known_output_but_reference_unknown: 11\n",
      "\n",
      "Continuous Outcomes Errors:\n",
      "known_output_but_reference_unknown: 88\n",
      "incorrect_output: 125\n",
      "badly_formatted_output: 229\n",
      "unknown_output_but_reference_known: 142\n",
      "\n",
      "--------------------------------------------------\n",
      "Model: pmc_llama\n",
      "--------------------------------------------------\n",
      "Outcome Type Errors:\n",
      "binary_when_continuous: 151\n",
      "badly_formatted_output: 13\n",
      "continuous_when_binary: 10\n",
      "unknown_when_reference_known: 2\n",
      "\n",
      "Binary Outcomes Errors:\n",
      "incorrect_output: 26\n",
      "unknown_output_but_reference_known: 146\n",
      "badly_formatted_output: 11\n",
      "known_output_but_reference_unknown: 3\n",
      "\n",
      "Continuous Outcomes Errors:\n",
      "unknown_output_but_reference_known: 431\n",
      "known_output_but_reference_unknown: 22\n",
      "incorrect_output: 60\n",
      "badly_formatted_output: 11\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def print_errors(errors: Dict, type: str):\n",
    "    print(f\"{type} Errors:\")\n",
    "    # print(json.dumps(errors, indent=4))\n",
    "    for key, value in errors.items():\n",
    "        print(f\"{key}: {len(value)}\")\n",
    "    print()\n",
    "\n",
    "for model, errors in outcome_type_errors_by_model.items():\n",
    "    print(f\"Model: {model}\")\n",
    "    print(\"-\" * 50)\n",
    "    print_errors(errors[\"outcome_type\"], \"Outcome Type\")\n",
    "    print_errors(errors[\"binary_outcomes\"], \"Binary Outcomes\")\n",
    "    print_errors(errors[\"continuous_outcomes\"], \"Continuous Outcomes\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b221e59d707dd3e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T02:16:23.261618Z",
     "start_time": "2024-04-11T02:16:23.258247Z"
    }
   },
   "outputs": [],
   "source": [
    "# Build a dictionary of the errors for each model\n",
    "random.seed(10)\n",
    "\n",
    "sampled_errors = {}\n",
    "for model in ['gpt4', 'mistral']:\n",
    "    outcome_errors = {}\n",
    "    for error_type, errors in outcome_type_errors_by_model[model].items():\n",
    "        for category, records in errors.items():\n",
    "            sample_size = min(20, len(records))\n",
    "            sampled_records = random.sample(records, sample_size)\n",
    "            outcome_errors[error_type] = outcome_errors.get(error_type, {})\n",
    "            outcome_errors[error_type][category] = sampled_records\n",
    "    sampled_errors[model] = outcome_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8fb84cc71984dc8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T02:16:23.283716Z",
     "start_time": "2024-04-11T02:16:23.263472Z"
    }
   },
   "outputs": [],
   "source": [
    "# load all the errors from gpt4 and mistral to a json file\n",
    "with open('sampled_errors.json', 'w') as f:\n",
    "    json.dump(sampled_errors, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
