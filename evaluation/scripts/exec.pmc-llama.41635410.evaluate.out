/home/yun.hy/.conda/envs/llm-meta-analysis/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Arguments for the Clinical Trials Meta Analysis Task Runner:
Model:        pmc-llama
Task:         binary_outcomes
Split:        test
Prompt Name:  yaml
Output Path:  /scratch/yun.hy/llm-meta-analysis/evaluation/outputs/binary_outcomes
Is Test:      None

Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:02<00:10,  2.09s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:04<00:08,  2.15s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:06<00:06,  2.18s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:08<00:04,  2.22s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:11<00:02,  2.22s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:11<00:00,  1.70s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:11<00:00,  1.95s/it]
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
  0%|          | 0/171 [00:00<?, ?it/s]100%|██████████| 171/171 [00:00<00:00, 2213.63it/s]
  0%|          | 0/171 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Token indices sequence length is longer than the specified maximum sequence length for this model (942 > 512). Running this sequence through the model will result in indexing errors
  1%|          | 1/171 [00:11<32:09, 11.35s/it]  1%|          | 2/171 [00:19<26:42,  9.48s/it]  2%|▏         | 3/171 [00:37<37:07, 13.26s/it]  2%|▏         | 4/171 [00:55<42:06, 15.13s/it]  3%|▎         | 5/171 [01:00<32:16, 11.67s/it]  4%|▎         | 6/171 [01:14<33:45, 12.28s/it]This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (2048). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.
  4%|▍         | 7/171 [01:27<34:23, 12.58s/it]  5%|▍         | 8/171 [02:10<1:00:06, 22.13s/it]  5%|▌         | 9/171 [02:19<49:04, 18.18s/it]    6%|▌         | 10/171 [02:33<45:04, 16.80s/it]  6%|▋         | 11/171 [02:42<38:36, 14.48s/it]  7%|▋         | 12/171 [02:55<37:27, 14.13s/it]  8%|▊         | 13/171 [03:33<55:50, 21.20s/it]  8%|▊         | 14/171 [04:06<1:05:21, 24.98s/it]  9%|▉         | 15/171 [04:19<55:31, 21.36s/it]    9%|▉         | 16/171 [04:44<57:18, 22.18s/it] 10%|▉         | 17/171 [05:02<53:59, 21.04s/it] 11%|█         | 18/171 [05:34<1:02:17, 24.42s/it] 11%|█         | 19/171 [06:14<1:13:33, 29.03s/it] 12%|█▏        | 20/171 [06:33<1:05:17, 25.94s/it] 12%|█▏        | 21/171 [06:37<48:24, 19.36s/it]   13%|█▎        | 22/171 [07:10<58:03, 23.38s/it] 13%|█▎        | 23/171 [07:32<57:02, 23.13s/it] 14%|█▍        | 24/171 [08:04<1:03:05, 25.75s/it] 15%|█▍        | 25/171 [08:33<1:05:20, 26.85s/it] 15%|█▌        | 26/171 [08:43<52:34, 21.76s/it]   16%|█▌        | 27/171 [09:07<53:56, 22.48s/it] 16%|█▋        | 28/171 [09:14<41:58, 17.62s/it] 17%|█▋        | 29/171 [09:50<54:43, 23.13s/it] 18%|█▊        | 30/171 [10:03<47:08, 20.06s/it] 18%|█▊        | 31/171 [10:14<40:49, 17.50s/it] 19%|█▊        | 32/171 [10:17<30:28, 13.16s/it] 19%|█▉        | 33/171 [10:27<27:53, 12.13s/it] 20%|█▉        | 34/171 [10:33<23:45, 10.41s/it] 20%|██        | 35/171 [10:52<29:36, 13.06s/it] 21%|██        | 36/171 [11:36<49:55, 22.19s/it] 22%|██▏       | 37/171 [12:07<55:18, 24.77s/it] 22%|██▏       | 38/171 [12:17<45:12, 20.39s/it] 23%|██▎       | 39/171 [12:31<40:51, 18.57s/it] 23%|██▎       | 40/171 [12:50<40:53, 18.73s/it] 24%|██▍       | 41/171 [13:18<46:25, 21.43s/it] 25%|██▍       | 42/171 [13:27<38:18, 17.82s/it] 25%|██▌       | 43/171 [13:50<41:13, 19.32s/it] 26%|██▌       | 44/171 [14:23<49:12, 23.24s/it] 26%|██▋       | 45/171 [15:02<59:13, 28.20s/it] 27%|██▋       | 46/171 [15:26<55:40, 26.72s/it] 27%|██▋       | 47/171 [15:34<43:38, 21.12s/it] 28%|██▊       | 48/171 [15:52<41:24, 20.20s/it] 29%|██▊       | 49/171 [16:10<39:36, 19.48s/it] 29%|██▉       | 50/171 [16:31<40:25, 20.05s/it] 30%|██▉       | 51/171 [16:38<32:33, 16.28s/it] 30%|███       | 52/171 [16:42<24:38, 12.42s/it] 31%|███       | 53/171 [17:05<31:00, 15.76s/it] 32%|███▏      | 54/171 [17:32<37:03, 19.00s/it] 32%|███▏      | 55/171 [17:54<38:31, 19.93s/it] 33%|███▎      | 56/171 [18:17<39:49, 20.77s/it] 33%|███▎      | 57/171 [18:45<43:52, 23.09s/it] 34%|███▍      | 58/171 [19:05<41:26, 22.01s/it] 35%|███▍      | 59/171 [19:18<36:18, 19.45s/it] 35%|███▌      | 60/171 [19:39<36:24, 19.68s/it] 36%|███▌      | 61/171 [19:58<35:41, 19.47s/it] 36%|███▋      | 62/171 [20:17<35:11, 19.37s/it] 37%|███▋      | 63/171 [20:41<37:26, 20.80s/it] 37%|███▋      | 64/171 [20:58<35:24, 19.86s/it] 38%|███▊      | 65/171 [21:22<37:00, 20.95s/it] 39%|███▊      | 66/171 [21:45<37:38, 21.51s/it] 39%|███▉      | 67/171 [22:00<34:02, 19.64s/it] 40%|███▉      | 68/171 [22:07<27:07, 15.80s/it] 40%|████      | 69/171 [23:11<51:36, 30.36s/it] 41%|████      | 70/171 [24:01<1:00:47, 36.11s/it] 42%|████▏     | 71/171 [24:28<55:54, 33.54s/it]   42%|████▏     | 72/171 [24:40<44:41, 27.09s/it] 43%|████▎     | 73/171 [24:51<36:02, 22.06s/it] 43%|████▎     | 74/171 [25:19<38:42, 23.94s/it] 44%|████▍     | 75/171 [25:47<40:28, 25.30s/it] 44%|████▍     | 76/171 [25:53<30:32, 19.29s/it] 45%|████▌     | 77/171 [26:10<29:08, 18.60s/it] 46%|████▌     | 78/171 [26:44<36:03, 23.27s/it] 46%|████▌     | 79/171 [26:54<29:26, 19.20s/it] 47%|████▋     | 80/171 [27:09<27:38, 18.22s/it] 47%|████▋     | 81/171 [27:40<32:58, 21.99s/it] 48%|████▊     | 82/171 [27:48<26:14, 17.69s/it] 49%|████▊     | 83/171 [28:13<29:00, 19.78s/it] 49%|████▉     | 84/171 [28:44<33:33, 23.15s/it] 50%|████▉     | 85/171 [29:06<32:43, 22.83s/it] 50%|█████     | 86/171 [29:13<25:53, 18.28s/it] 51%|█████     | 87/171 [29:27<23:36, 16.86s/it] 51%|█████▏    | 88/171 [29:32<18:26, 13.34s/it] 52%|█████▏    | 89/171 [29:53<21:13, 15.53s/it] 53%|█████▎    | 90/171 [30:17<24:23, 18.06s/it] 53%|█████▎    | 91/171 [30:27<21:08, 15.86s/it] 54%|█████▍    | 92/171 [30:42<20:12, 15.35s/it] 54%|█████▍    | 93/171 [30:48<16:38, 12.80s/it] 55%|█████▍    | 94/171 [31:15<21:45, 16.96s/it] 56%|█████▌    | 95/171 [31:37<23:20, 18.43s/it] 56%|█████▌    | 96/171 [32:12<29:18, 23.45s/it] 57%|█████▋    | 97/171 [32:23<24:18, 19.70s/it] 57%|█████▋    | 98/171 [32:30<19:21, 15.91s/it] 58%|█████▊    | 99/171 [33:12<28:20, 23.62s/it] 58%|█████▊    | 100/171 [33:34<27:22, 23.14s/it] 59%|█████▉    | 101/171 [34:15<33:26, 28.66s/it] 60%|█████▉    | 102/171 [34:22<25:23, 22.09s/it] 60%|██████    | 103/171 [34:51<27:15, 24.05s/it] 61%|██████    | 104/171 [35:16<27:08, 24.31s/it] 61%|██████▏   | 105/171 [35:27<22:24, 20.37s/it] 62%|██████▏   | 106/171 [35:57<25:13, 23.28s/it] 63%|██████▎   | 107/171 [36:16<23:30, 22.04s/it] 63%|██████▎   | 108/171 [36:33<21:37, 20.60s/it] 64%|██████▎   | 109/171 [37:19<28:57, 28.02s/it] 64%|██████▍   | 110/171 [37:41<26:39, 26.22s/it] 65%|██████▍   | 111/171 [37:53<22:10, 22.17s/it] 65%|██████▌   | 112/171 [37:57<16:15, 16.53s/it] 66%|██████▌   | 113/171 [38:04<13:23, 13.86s/it] 67%|██████▋   | 114/171 [38:24<14:45, 15.54s/it] 67%|██████▋   | 115/171 [38:28<11:26, 12.25s/it] 68%|██████▊   | 116/171 [38:48<13:11, 14.39s/it] 68%|██████▊   | 117/171 [39:23<18:31, 20.58s/it] 69%|██████▉   | 118/171 [40:08<24:45, 28.03s/it] 70%|██████▉   | 119/171 [40:22<20:30, 23.66s/it] 70%|███████   | 120/171 [40:43<19:25, 22.85s/it] 71%|███████   | 121/171 [41:06<19:08, 22.97s/it] 71%|███████▏  | 122/171 [41:42<22:05, 27.06s/it] 72%|███████▏  | 123/171 [41:52<17:29, 21.87s/it] 73%|███████▎  | 124/171 [42:13<16:46, 21.42s/it] 73%|███████▎  | 125/171 [42:18<12:51, 16.76s/it] 74%|███████▎  | 126/171 [42:28<10:59, 14.66s/it] 74%|███████▍  | 127/171 [42:54<13:15, 18.07s/it] 75%|███████▍  | 128/171 [43:00<10:23, 14.50s/it] 75%|███████▌  | 129/171 [43:37<14:52, 21.25s/it] 76%|███████▌  | 130/171 [43:54<13:38, 19.97s/it] 77%|███████▋  | 131/171 [44:14<13:12, 19.82s/it] 77%|███████▋  | 132/171 [44:41<14:16, 21.97s/it] 78%|███████▊  | 133/171 [44:55<12:22, 19.54s/it] 78%|███████▊  | 134/171 [45:02<09:51, 15.98s/it] 79%|███████▉  | 135/171 [45:19<09:38, 16.06s/it] 80%|███████▉  | 136/171 [45:31<08:44, 14.99s/it] 80%|████████  | 137/171 [46:07<12:07, 21.39s/it] 81%|████████  | 138/171 [46:29<11:45, 21.37s/it] 81%|████████▏ | 139/171 [46:34<08:46, 16.44s/it] 82%|████████▏ | 140/171 [47:08<11:18, 21.87s/it] 82%|████████▏ | 141/171 [47:16<08:48, 17.61s/it] 83%|████████▎ | 142/171 [47:42<09:40, 20.03s/it] 84%|████████▎ | 143/171 [47:47<07:20, 15.72s/it] 84%|████████▍ | 144/171 [48:02<06:54, 15.35s/it] 85%|████████▍ | 145/171 [48:33<08:45, 20.19s/it] 85%|████████▌ | 146/171 [48:40<06:43, 16.15s/it] 86%|████████▌ | 147/171 [49:19<09:11, 22.98s/it] 87%|████████▋ | 148/171 [50:14<12:29, 32.60s/it] 87%|████████▋ | 149/171 [50:23<09:25, 25.70s/it] 88%|████████▊ | 150/171 [51:14<11:38, 33.26s/it] 88%|████████▊ | 151/171 [51:26<08:57, 26.87s/it] 89%|████████▉ | 152/171 [51:34<06:41, 21.12s/it] 89%|████████▉ | 153/171 [52:12<07:50, 26.14s/it] 90%|█████████ | 154/171 [52:37<07:17, 25.76s/it] 91%|█████████ | 155/171 [52:53<06:07, 22.96s/it] 91%|█████████ | 156/171 [53:21<06:08, 24.54s/it] 92%|█████████▏| 157/171 [53:26<04:17, 18.40s/it] 92%|█████████▏| 158/171 [53:45<04:01, 18.60s/it] 93%|█████████▎| 159/171 [54:02<03:38, 18.21s/it] 94%|█████████▎| 160/171 [54:23<03:31, 19.23s/it] 94%|█████████▍| 161/171 [54:36<02:52, 17.20s/it] 95%|█████████▍| 162/171 [54:41<02:02, 13.63s/it] 95%|█████████▌| 163/171 [54:48<01:32, 11.58s/it] 96%|█████████▌| 164/171 [55:32<02:28, 21.23s/it] 96%|█████████▋| 165/171 [55:42<01:48, 18.03s/it] 97%|█████████▋| 166/171 [56:00<01:29, 17.83s/it] 98%|█████████▊| 167/171 [56:20<01:13, 18.49s/it] 98%|█████████▊| 168/171 [56:45<01:01, 20.47s/it] 99%|█████████▉| 169/171 [57:10<00:43, 21.72s/it] 99%|█████████▉| 170/171 [58:02<00:30, 30.86s/it]100%|██████████| 171/171 [59:01<00:00, 39.33s/it]100%|██████████| 171/171 [59:01<00:00, 20.71s/it]
ERROR - chunk to combine is too long: 1766 tokens
ERROR - chunk to combine is too long: 2356 tokens
ERROR - chunk to combine is too long: 2360 tokens
ERROR - chunk to combine is too long: 1766 tokens
ERROR - chunk to combine is too long: 2356 tokens
ERROR - chunk to combine is too long: 2360 tokens
ERROR - chunk to combine is too long: 2010 tokens
ERROR - chunk to combine is too long: 2362 tokens
ERROR - chunk to combine is too long: 2475 tokens
ERROR - chunk to combine is too long: 2531 tokens
ERROR - chunk to combine is too long: 2010 tokens
ERROR - chunk to combine is too long: 2362 tokens
ERROR - chunk to combine is too long: 2475 tokens
ERROR - chunk to combine is too long: 2531 tokens
ERROR - chunk to combine is too long: 2010 tokens
ERROR - chunk to combine is too long: 2362 tokens
ERROR - chunk to combine is too long: 2475 tokens
ERROR - chunk to combine is too long: 2531 tokens
ERROR - chunk to combine is too long: 1766 tokens
ERROR - chunk to combine is too long: 2356 tokens
ERROR - chunk to combine is too long: 2360 tokens
ERROR - chunk to combine is too long: 1766 tokens
ERROR - chunk to combine is too long: 2356 tokens
ERROR - chunk to combine is too long: 2360 tokens
ERROR - chunk to combine is too long: 1766 tokens
ERROR - chunk to combine is too long: 2356 tokens
ERROR - chunk to combine is too long: 2360 tokens
ERROR - chunk to combine is too long: 1766 tokens
ERROR - chunk to combine is too long: 2356 tokens
ERROR - chunk to combine is too long: 2360 tokens
ERROR - chunk to combine is too long: 1766 tokens
ERROR - chunk to combine is too long: 2356 tokens
ERROR - chunk to combine is too long: 2360 tokens
ERROR - chunk to combine is too long: 1766 tokens
ERROR - chunk to combine is too long: 2356 tokens
ERROR - chunk to combine is too long: 2360 tokens
ERROR - chunk to combine is too long: 1818 tokens
ERROR - chunk to combine is too long: 1752 tokens
ERROR - chunk to combine is too long: 2010 tokens
ERROR - chunk to combine is too long: 2362 tokens
ERROR - chunk to combine is too long: 2475 tokens
ERROR - chunk to combine is too long: 2531 tokens
ERROR - chunk to combine is too long: 2010 tokens
ERROR - chunk to combine is too long: 2362 tokens
ERROR - chunk to combine is too long: 2475 tokens
ERROR - chunk to combine is too long: 2531 tokens
Saving outputs for task - binary_outcomes; prompt - yaml; model - pmc-llama to csv and json
Task outputs saved to /scratch/yun.hy/llm-meta-analysis/evaluation/outputs/binary_outcomes/pmc-llama_binary_outcomes_test_output_20240331-22:04:16.json and /scratch/yun.hy/llm-meta-analysis/evaluation/outputs/binary_outcomes/pmc-llama_binary_outcomes_test_output_20240331-22:04:16.csv
/home/yun.hy/.conda/envs/llm-meta-analysis/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Arguments for the Clinical Trials Meta Analysis Task Runner:
Model:        pmc-llama
Task:         continuous_outcomes
Split:        test
Prompt Name:  yaml
Output Path:  /scratch/yun.hy/llm-meta-analysis/evaluation/outputs/continuous_outcomes
Is Test:      None

Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:01<00:09,  2.00s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:04<00:08,  2.10s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:06<00:06,  2.14s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:08<00:04,  2.18s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:10<00:02,  2.19s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:11<00:00,  1.67s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:11<00:00,  1.91s/it]
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
  0%|          | 0/485 [00:00<?, ?it/s] 44%|████▎     | 212/485 [00:00<00:00, 2110.54it/s] 88%|████████▊ | 427/485 [00:00<00:00, 2130.84it/s]100%|██████████| 485/485 [00:00<00:00, 2129.56it/s]
  0%|          | 0/485 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Token indices sequence length is longer than the specified maximum sequence length for this model (1780 > 512). Running this sequence through the model will result in indexing errors
  0%|          | 1/485 [00:08<1:06:01,  8.18s/it]  0%|          | 2/485 [00:38<2:52:42, 21.45s/it]This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (2048). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.
  0%|          | 2/485 [00:50<3:23:02, 25.22s/it]
ERROR - chunk to combine is too long: 1765 tokens
ERROR - chunk to combine is too long: 1742 tokens
ERROR - chunk to combine is too long: 1742 tokens
ERROR - chunk to combine is too long: 1718 tokens
[ERROR] The size of tensor a (2048) must match the size of tensor b (2049) at non-singleton dimension 3
Traceback (most recent call last):
  File "/scratch/yun.hy/llm-meta-analysis/evaluation/scripts/../run_task.py", line 330, in <module>
    json_file_path, csv_file_path = task_runner.run_task()
                                    ^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/yun.hy/llm-meta-analysis/evaluation/scripts/../run_task.py", line 228, in run_task
    concatenated_output = concatenated_output + output + "\n---\n"
                          ~~~~~~~~~~~~~~~~~~~~^~~~~~~~
TypeError: can only concatenate str (not "NoneType") to str
