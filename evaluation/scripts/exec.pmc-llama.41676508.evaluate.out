/home/yun.hy/.conda/envs/llm-meta-analysis/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Arguments for the Clinical Trials Meta Analysis Task Runner:
Model:        pmc-llama
Task:         binary_outcomes
Split:        test
Prompt Name:  yaml
Output Path:  /scratch/yun.hy/llm-meta-analysis/evaluation/outputs/binary_outcomes
Is Test:      None

Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:02<00:14,  2.94s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:05<00:11,  2.83s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:08<00:08,  2.95s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:11<00:05,  2.82s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:14<00:02,  2.75s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:14<00:00,  2.06s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:14<00:00,  2.46s/it]
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
  0%|          | 0/171 [00:00<?, ?it/s]100%|██████████| 171/171 [00:00<00:00, 2232.74it/s]
  0%|          | 0/171 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Token indices sequence length is longer than the specified maximum sequence length for this model (1826 > 512). Running this sequence through the model will result in indexing errors
  1%|          | 1/171 [00:10<29:28, 10.40s/it]  1%|          | 2/171 [00:18<25:38,  9.10s/it]  2%|▏         | 3/171 [00:29<27:53,  9.96s/it]  2%|▏         | 4/171 [00:45<34:32, 12.41s/it]  3%|▎         | 5/171 [00:51<27:32,  9.96s/it]  4%|▎         | 6/171 [01:03<29:22, 10.68s/it]  4%|▍         | 7/171 [01:08<23:53,  8.74s/it]  5%|▍         | 8/171 [01:28<33:21, 12.28s/it]  5%|▌         | 9/171 [01:33<27:30, 10.19s/it]  6%|▌         | 10/171 [01:38<23:16,  8.68s/it]  6%|▋         | 11/171 [01:43<20:12,  7.58s/it]  7%|▋         | 12/171 [01:53<21:58,  8.30s/it]  8%|▊         | 13/171 [02:16<32:54, 12.49s/it]  8%|▊         | 14/171 [02:34<37:08, 14.20s/it]  9%|▉         | 15/171 [02:42<31:58, 12.30s/it]  9%|▉         | 16/171 [02:53<31:18, 12.12s/it] 10%|▉         | 17/171 [03:00<26:40, 10.39s/it] 11%|█         | 18/171 [03:16<30:53, 12.11s/it] 11%|█         | 19/171 [03:37<37:54, 14.96s/it] 12%|█▏        | 20/171 [03:50<35:46, 14.22s/it] 12%|█▏        | 21/171 [03:55<28:42, 11.49s/it]This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (2048). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.
 13%|█▎        | 22/171 [04:33<47:55, 19.30s/it] 13%|█▎        | 23/171 [04:48<45:03, 18.27s/it] 14%|█▍        | 24/171 [05:04<43:01, 17.56s/it] 15%|█▍        | 25/171 [05:27<46:46, 19.22s/it] 15%|█▌        | 26/171 [05:35<38:03, 15.75s/it] 16%|█▌        | 27/171 [05:51<38:01, 15.84s/it] 16%|█▋        | 28/171 [05:56<29:57, 12.57s/it] 17%|█▋        | 29/171 [06:23<40:04, 16.93s/it] 18%|█▊        | 30/171 [06:33<34:35, 14.72s/it] 18%|█▊        | 31/171 [06:45<32:21, 13.87s/it] 19%|█▊        | 32/171 [06:47<24:19, 10.50s/it] 19%|█▉        | 33/171 [06:52<19:57,  8.67s/it] 20%|█▉        | 34/171 [06:58<18:07,  7.94s/it] 20%|██        | 35/171 [07:13<22:55, 10.11s/it] 21%|██        | 36/171 [07:37<32:15, 14.34s/it] 22%|██▏       | 37/171 [07:57<35:28, 15.89s/it] 22%|██▏       | 38/171 [08:06<30:52, 13.93s/it] 23%|██▎       | 39/171 [08:15<27:00, 12.27s/it] 23%|██▎       | 40/171 [08:26<25:58, 11.89s/it] 24%|██▍       | 41/171 [08:39<26:48, 12.37s/it] 25%|██▍       | 42/171 [08:45<22:40, 10.55s/it] 25%|██▌       | 43/171 [09:00<25:01, 11.73s/it] 26%|██▌       | 44/171 [09:25<33:15, 15.71s/it] 26%|██▋       | 45/171 [10:01<46:12, 22.00s/it] 27%|██▋       | 46/171 [10:16<41:00, 19.68s/it] 27%|██▋       | 47/171 [10:22<32:18, 15.63s/it] 28%|██▊       | 48/171 [10:59<45:14, 22.07s/it] 29%|██▊       | 49/171 [11:11<38:26, 18.90s/it] 29%|██▉       | 50/171 [11:32<39:38, 19.66s/it] 30%|██▉       | 51/171 [11:36<30:05, 15.05s/it] 30%|███       | 52/171 [11:42<24:15, 12.23s/it] 31%|███       | 53/171 [11:56<25:03, 12.74s/it] 32%|███▏      | 54/171 [12:13<27:43, 14.22s/it] 32%|███▏      | 55/171 [12:28<27:34, 14.26s/it] 33%|███▎      | 56/171 [13:00<37:41, 19.67s/it] 33%|███▎      | 57/171 [13:19<37:01, 19.48s/it] 34%|███▍      | 58/171 [13:31<32:24, 17.21s/it] 35%|███▍      | 59/171 [13:41<27:47, 14.89s/it] 35%|███▌      | 60/171 [13:52<25:26, 13.75s/it] 36%|███▌      | 61/171 [14:07<25:54, 14.14s/it] 36%|███▋      | 62/171 [14:19<24:31, 13.50s/it] 37%|███▋      | 63/171 [14:33<24:26, 13.58s/it] 37%|███▋      | 64/171 [14:43<22:39, 12.70s/it] 38%|███▊      | 65/171 [15:01<25:13, 14.28s/it] 39%|███▊      | 66/171 [15:34<34:34, 19.75s/it] 39%|███▉      | 67/171 [15:40<27:11, 15.68s/it] 40%|███▉      | 68/171 [15:45<21:38, 12.60s/it] 40%|████      | 69/171 [16:22<33:51, 19.92s/it] 41%|████      | 70/171 [17:01<43:12, 25.67s/it] 42%|████▏     | 71/171 [17:21<39:54, 23.95s/it] 42%|████▏     | 72/171 [17:31<32:30, 19.70s/it] 43%|████▎     | 73/171 [17:37<25:31, 15.63s/it] 43%|████▎     | 74/171 [17:59<28:10, 17.42s/it] 44%|████▍     | 75/171 [18:20<29:43, 18.58s/it] 44%|████▍     | 76/171 [18:23<21:53, 13.82s/it] 45%|████▌     | 77/171 [18:33<19:57, 12.74s/it] 46%|████▌     | 78/171 [18:53<23:17, 15.03s/it] 46%|████▌     | 79/171 [19:00<19:19, 12.61s/it] 47%|████▋     | 80/171 [19:11<18:12, 12.01s/it] 47%|████▋     | 81/171 [19:31<21:29, 14.32s/it] 48%|████▊     | 82/171 [19:36<17:09, 11.56s/it] 49%|████▊     | 83/171 [19:56<20:54, 14.25s/it] 49%|████▉     | 84/171 [20:15<22:32, 15.54s/it] 50%|████▉     | 85/171 [20:33<23:26, 16.36s/it] 50%|█████     | 86/171 [20:37<17:40, 12.47s/it] 51%|█████     | 87/171 [20:44<15:12, 10.86s/it] 51%|█████▏    | 88/171 [20:50<12:58,  9.38s/it] 52%|█████▏    | 89/171 [21:04<14:48, 10.84s/it] 53%|█████▎    | 90/171 [21:20<16:55, 12.54s/it] 53%|█████▎    | 91/171 [21:28<14:47, 11.09s/it] 54%|█████▍    | 92/171 [21:40<14:45, 11.21s/it] 54%|█████▍    | 93/171 [21:45<12:21,  9.50s/it] 55%|█████▍    | 94/171 [22:02<15:10, 11.83s/it] 56%|█████▌    | 95/171 [22:25<19:11, 15.16s/it] 56%|█████▌    | 96/171 [22:57<25:01, 20.03s/it] 57%|█████▋    | 97/171 [23:04<20:10, 16.35s/it] 57%|█████▋    | 98/171 [23:10<16:06, 13.24s/it] 58%|█████▊    | 99/171 [23:49<25:12, 21.01s/it] 58%|█████▊    | 100/171 [24:09<24:29, 20.69s/it] 59%|█████▉    | 101/171 [24:34<25:24, 21.77s/it] 60%|█████▉    | 102/171 [24:41<20:05, 17.47s/it] 60%|██████    | 103/171 [24:53<18:00, 15.89s/it] 61%|██████    | 104/171 [25:07<16:54, 15.14s/it] 61%|██████▏   | 105/171 [25:14<14:10, 12.89s/it] 62%|██████▏   | 106/171 [25:30<14:49, 13.68s/it] 63%|██████▎   | 107/171 [25:47<15:49, 14.83s/it] 63%|██████▎   | 108/171 [25:59<14:38, 13.95s/it] 64%|██████▎   | 109/171 [26:33<20:35, 19.93s/it] 64%|██████▍   | 110/171 [26:52<19:46, 19.46s/it] 65%|██████▍   | 111/171 [27:01<16:34, 16.58s/it] 65%|██████▌   | 112/171 [27:05<12:21, 12.56s/it] 66%|██████▌   | 113/171 [27:14<11:16, 11.67s/it] 67%|██████▋   | 114/171 [27:32<12:45, 13.44s/it] 67%|██████▋   | 115/171 [27:37<10:17, 11.03s/it] 68%|██████▊   | 116/171 [27:45<09:19, 10.17s/it] 68%|██████▊   | 117/171 [28:26<17:25, 19.36s/it] 69%|██████▉   | 118/171 [29:05<22:21, 25.32s/it] 70%|██████▉   | 119/171 [29:12<16:59, 19.60s/it] 70%|███████   | 120/171 [29:27<15:35, 18.34s/it] 71%|███████   | 121/171 [29:50<16:23, 19.68s/it] 71%|███████▏  | 122/171 [30:15<17:32, 21.47s/it] 72%|███████▏  | 123/171 [30:23<13:52, 17.35s/it] 73%|███████▎  | 124/171 [30:39<13:08, 16.77s/it] 73%|███████▎  | 125/171 [30:45<10:28, 13.67s/it] 74%|███████▎  | 126/171 [30:56<09:36, 12.81s/it] 74%|███████▍  | 127/171 [31:18<11:23, 15.54s/it] 75%|███████▍  | 128/171 [31:23<08:50, 12.33s/it] 75%|███████▌  | 129/171 [32:01<14:01, 20.04s/it] 76%|███████▌  | 130/171 [32:11<11:38, 17.04s/it] 77%|███████▋  | 131/171 [32:22<10:08, 15.21s/it] 77%|███████▋  | 132/171 [32:38<10:09, 15.62s/it] 78%|███████▊  | 133/171 [32:50<09:05, 14.36s/it] 78%|███████▊  | 134/171 [32:56<07:18, 11.84s/it] 79%|███████▉  | 135/171 [33:02<06:10, 10.30s/it] 80%|███████▉  | 136/171 [33:09<05:21,  9.18s/it] 80%|████████  | 137/171 [33:29<07:08, 12.61s/it] 81%|████████  | 138/171 [33:47<07:45, 14.09s/it] 81%|████████▏ | 139/171 [33:51<05:57, 11.17s/it] 82%|████████▏ | 140/171 [34:16<07:46, 15.06s/it] 82%|████████▏ | 141/171 [34:26<06:53, 13.77s/it] 83%|████████▎ | 142/171 [34:43<07:03, 14.61s/it] 84%|████████▎ | 143/171 [34:50<05:49, 12.49s/it] 84%|████████▍ | 144/171 [35:02<05:32, 12.32s/it] 85%|████████▍ | 145/171 [35:23<06:22, 14.71s/it] 85%|████████▌ | 146/171 [35:28<04:58, 11.94s/it] 86%|████████▌ | 147/171 [36:03<07:30, 18.77s/it] 87%|████████▋ | 148/171 [36:55<11:05, 28.94s/it] 87%|████████▋ | 149/171 [37:07<08:39, 23.60s/it] 88%|████████▊ | 150/171 [37:43<09:35, 27.41s/it] 88%|████████▊ | 151/171 [37:54<07:30, 22.50s/it] 89%|████████▉ | 152/171 [38:01<05:42, 18.00s/it] 89%|████████▉ | 153/171 [38:19<05:23, 17.95s/it] 90%|█████████ | 154/171 [38:37<05:03, 17.86s/it] 91%|█████████ | 155/171 [39:11<06:01, 22.61s/it] 91%|█████████ | 156/171 [39:34<05:40, 22.69s/it] 92%|█████████▏| 157/171 [39:38<03:59, 17.09s/it] 92%|█████████▏| 158/171 [39:55<03:43, 17.16s/it] 93%|█████████▎| 159/171 [40:05<02:59, 14.96s/it] 94%|█████████▎| 160/171 [40:16<02:31, 13.81s/it] 94%|█████████▍| 161/171 [40:22<01:56, 11.64s/it] 95%|█████████▍| 162/171 [40:26<01:22,  9.15s/it] 95%|█████████▌| 163/171 [40:32<01:06,  8.32s/it] 96%|█████████▌| 164/171 [41:08<01:56, 16.62s/it] 96%|█████████▋| 165/171 [41:19<01:29, 14.99s/it] 97%|█████████▋| 166/171 [41:33<01:12, 14.58s/it] 98%|█████████▊| 167/171 [42:11<01:26, 21.56s/it] 98%|█████████▊| 168/171 [42:25<00:58, 19.36s/it] 99%|█████████▉| 169/171 [42:36<00:34, 17.00s/it] 99%|█████████▉| 170/171 [43:13<00:22, 22.92s/it]100%|██████████| 171/171 [43:55<00:00, 28.66s/it]100%|██████████| 171/171 [43:55<00:00, 15.41s/it]
Saving outputs for task - binary_outcomes; prompt - yaml; model - pmc-llama to csv and json
Task outputs saved to /scratch/yun.hy/llm-meta-analysis/evaluation/outputs/binary_outcomes/pmc-llama_binary_outcomes_test_output_20240404-07:06:18.json and /scratch/yun.hy/llm-meta-analysis/evaluation/outputs/binary_outcomes/pmc-llama_binary_outcomes_test_output_20240404-07:06:18.csv
