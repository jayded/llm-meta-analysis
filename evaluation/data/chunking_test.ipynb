{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b02a5a4cdbe170b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-23T03:06:10.898255Z",
     "start_time": "2024-03-23T03:06:10.894015Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from copy import deepcopy\n",
    "import tiktoken\n",
    "from pprint import pprint\n",
    "from IPython.display import clear_output\n",
    "from copy import copy\n",
    "\n",
    "RAW_XML_DIR = 'abstract_and_results_xml_files'\n",
    "NO_ATTR_XML_DIR = 'no_attributes_xml_files'\n",
    "DATASET_DIR = 'annotated_rct_dataset.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "173aba4fd612f7ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-23T04:59:28.713048Z",
     "start_time": "2024-03-23T04:59:28.698623Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_html_body(soup):\n",
    "    \"\"\"Given a BeautifulSoup object, remove the html and body tags\"\"\"\n",
    "    html_tag = soup.html\n",
    "    body_tag = soup.body\n",
    "\n",
    "    # Unwrap the unnecessary tags that are added by lxml parser\n",
    "    if html_tag is not None:\n",
    "        html_tag.unwrap()\n",
    "    if body_tag is not None:\n",
    "        body_tag.unwrap()\n",
    "\n",
    "    return soup\n",
    "\n",
    "def read_xml_directory(directory):\n",
    "    \"\"\" Read all the XML files in the directory and return a dictionary with pmcid as the key and BeautifulSoup object as the value\"\"\"\n",
    "    soups = dict()\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".xml\"):\n",
    "            # Get the pmcid from the filename\n",
    "            pmcid = int(filename.split('.')[0].split('C')[1])\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            with open(filepath, 'r') as file:\n",
    "                soup = BeautifulSoup(file.read(), 'lxml')\n",
    "\n",
    "                # Remove the html and body tags\n",
    "                remove_html_body(soup)\n",
    "\n",
    "                # Add the soup to the dictionary with pmcid as the key\n",
    "                soups[pmcid] = soup\n",
    "\n",
    "    return soups\n",
    "\n",
    "def convert_to_int(value):\n",
    "    \"\"\"Convert str value to an integer\"\"\"\n",
    "    if ',' in str(value):\n",
    "        return int(value.replace(',', ''))\n",
    "    else:\n",
    "        return int(value)\n",
    "\n",
    "def count_tokens(soup, encoding):\n",
    "    \"\"\"Given a soup object, return the number of tokens in the text\"\"\"\n",
    "    return len(encoding.encode(str(soup)))\n",
    "\n",
    "def condition():\n",
    "    \"\"\"Given a chunk of text, return True if the chunk meets the condition\"\"\"\n",
    "    \"\"\"\n",
    "    CHATGPT PROMPT:\n",
    "\n",
    "    You are an expert on medical randomized controlled trials. You are trying to extract any relevant values for meta-analysis: intervention events, intervention group size, comparator events, comparator group size, intervention mean, intervention standard deviation, comparator mean, comparator standard deviation. Output only \"y\" if any of these values exists within the given chunk, output only \"n\" if the chunk contains none of these relevant values. Do not provide any explanation\n",
    "\n",
    "    Intervention: {INTERVENTION}\n",
    "    Comparator: {COMPARATOR}\n",
    "    Outcome: {OUTCOME}\n",
    "\n",
    "    Chunk:\n",
    "    \"\"\"\n",
    "    return input('Is the model gonna return y or n (y/n) ') == 'y'\n",
    "\n",
    "def concatenate_soups(soup_list):\n",
    "    \"\"\"Concatenate a list of soup objects into a single soup object\"\"\"\n",
    "    new_soup = BeautifulSoup(\"\", 'lxml')\n",
    "    for soup in soup_list:\n",
    "        new_soup.append(copy(soup))\n",
    "    return new_soup\n",
    "\n",
    "def chunk_xml(xml_element, min_tokens, condition):\n",
    "    \"\"\"\n",
    "    Chunk the XML element into smaller parts based on the specified condition and minimum number of tokens for a valid chunk.\n",
    "    \"\"\"\n",
    "    keep_chunks = []\n",
    "\n",
    "    def process_chunk(chunk):\n",
    "        \"\"\"\n",
    "        Process a chunk: If the condition is true and chunk length is greater than min_tokens, further chunk it recursively.\n",
    "        \"\"\"\n",
    "\n",
    "        # Check if the condition is true for the chunk and chunk length is greater than min_tokens\n",
    "        chunk = deepcopy(chunk)\n",
    "        pprint(chunk)\n",
    "        relevant = condition()\n",
    "\n",
    "        is_p_tag = chunk.name == 'p'\n",
    "\n",
    "        try: is_table = 'table' in chunk.name\n",
    "        except TypeError: is_table = False\n",
    "\n",
    "        clear_output()\n",
    "\n",
    "        if (is_table and relevant) or (is_p_tag and relevant):\n",
    "            keep_chunks.append(chunk)\n",
    "\n",
    "        elif count_tokens(chunk, encoding) >= min_tokens and relevant and not is_table:\n",
    "            # Chunk it further, recursively\n",
    "            keep_chunks.extend(chunk_xml(chunk, min_tokens, condition))\n",
    "\n",
    "        # if the chunk is too small and the condition is true, keep it    \n",
    "        elif count_tokens(chunk, encoding) < min_tokens and relevant:\n",
    "            keep_chunks.extend(chunk)\n",
    "\n",
    "        # discard the chunk if the condition is false\n",
    "\n",
    "    # Iterate through the children of the XML element\n",
    "    for child in xml_element.contents:\n",
    "        # Process the chunk\n",
    "        process_chunk(child)  \n",
    "\n",
    "    # Return the list of chunks as a single soup object\n",
    "    return keep_chunks\n",
    "\n",
    "def combine_chunks(soup_list, max_length, count_tokens, encoding):\n",
    "    final_chunks = []\n",
    "    current_chunk = BeautifulSoup(\"\", 'lxml')\n",
    "    current_length = 0\n",
    "\n",
    "    for soup in soup_list:\n",
    "        soup_length = count_tokens(soup, encoding)\n",
    "        if current_length + soup_length > max_length:\n",
    "            # If adding this soup would exceed max_length, finish the current chunk\n",
    "            if current_length > 0:  # Avoid adding empty chunks\n",
    "                final_chunks.append(current_chunk)\n",
    "            # Start a new chunk with the current soup\n",
    "            current_chunk = soup\n",
    "            current_length = soup_length\n",
    "        else:\n",
    "            # If adding this soup wouldn't exceed max_length, add it to the current chunk\n",
    "            current_chunk.append(soup)\n",
    "            current_length += soup_length\n",
    "\n",
    "    # After the loop, add the last chunk if it's not empty\n",
    "    if current_length > 0:\n",
    "        final_chunks.append(current_chunk)\n",
    "\n",
    "    return final_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f590c0c377474632",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soups = read_xml_directory(NO_ATTR_XML_DIR)\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "# test_soup = soups[5498715]\n",
    "test_soup = soups[57750]\n",
    "print(count_tokens(test_soup, encoding))\n",
    "chunks = chunk_xml(test_soup, 250, condition)\n",
    "condensed_chunks = combine_chunks(chunks, 2000, count_tokens, encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55d9d754d9e3b86a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-23T04:52:03.654074Z",
     "start_time": "2024-03-23T04:52:03.638554Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[144]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[count_tokens(chunk, encoding) for chunk in condensed_chunks]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
