# Models

This document lists some details for each model used as part of this project. All the models we used are instruction-tuned for zero-shot inference. OLMo, PMC LLaMA, and BioMistral have been trained on scientific documents/texts that are more closely aligned to our input texts.

| Model | Training Data | Sequence Length | Model Weights | Details |
|---|---|---|---|---|
| GPT 3.5 | Common Crawl, webtexts, books, and Wikipedia | 16385 | OpenAI API | Model developed by OpenAI that can understand and generate natural language or code and have been optimized <br>for chat but work well for non-chat tasks as well. Trained using Reinforcement Learning from Human Feedback (RLHF). |
| GPT 4 |  | 128000 | OpenAI API | GPT-4 is a large multimodal model (accepting text or image inputs and outputting text) developed by OpenAI. |
| Mistral 7B Instruct v2 |  | 8192 | [Mistral-7B-Instruct-v0.2](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2) | Mistral 7B Instruct v2 is an instruct fine-tuned version of the Mistral-7B-v0.1 generative text model using <br>a variety of publicly available conversation datasets. This is the second version. |
| Gemma 7B Instruct | primarily-English data from web documents, mathematics, and code | 8192 | [Gemma](https://github.com/google-deepmind/gemma) | Gemma is an open-weights Large Language Model (LLM) by Google DeepMind, based on Gemini research and technology. |
| OLMo 7B Instruct | [Tulu 2 SFT Mix](https://huggingface.co/datasets/allenai/tulu-v2-sft-mixture) + <br>[Ultrafeedback Cleaned](https://huggingface.co/datasets/allenai/ultrafeedback_binarized_cleaned) | 2048 | [OLMo](https://allenai.org/olmo) | Open Language Model (OLMo) is developed by AI2 as an LLM framework to provide access to data, training code, models, <br>and evaluation code necessary to advance AI through open research to empower academics and researchers to study the <br>science of language models. |
| PMC LLaMA 13B | [S2ORC](https://github.com/allenai/s2orc) + <br>[PMC LLaMA instructions](https://huggingface.co/datasets/axiong/pmc_llama_instructions) | 2048 | [PMC-LLaMA](https://github.com/chaoyi-wu/PMC-LLaMA/tree/main) | PMC LLaMA 13B is initialized from LLaMA-13B and further pretrained with medical corpus. <br>Then, the model tuned with the instructions following the dataset. |
| BioMistral | [PubMed Central Open Access Subset](https://www.ncbi.nlm.nih.gov/pmc/tools/openftlist/) | 2048 | [BioMistral](https://huggingface.co/BioMistral/BioMistral-7B) | BioMistral is a Mistral-based further pre-trained open source model suited for the medical domains and pre-trained <br>using textual data from PubMed Central Open Access. |